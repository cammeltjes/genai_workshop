{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ur8xi4C7S06n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a SERPAPI_API_KEY on https://serpapi.com/ for free to use (amongst others) the Google Search API\n",
    "import os\n",
    "os.environ['SERPAPI_API_KEY']=#YOUR API KEY HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e",
    "tags": []
   },
   "source": [
    "# Prompt Design - Best Practices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook covers the essentials of prompt engineering, including some best practices.\n",
    "\n",
    "Learn more about prompt design in the [official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/text/text-overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this notebook, you learn best practices around prompt engineering -- how to design prompts to improve the quality of your responses.\n",
    "\n",
    "This notebook covers the following best practices for prompt engineering:\n",
    "\n",
    "- Be concise\n",
    "- Be specific and well-defined\n",
    "- Ask one task at a time\n",
    "- Turn generative tasks into classification tasks\n",
    "- Improve response quality by including examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea013f50403c"
   },
   "source": [
    "### Costs\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI Generative AI Studio\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
    "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e663cb43fa0"
   },
   "source": [
    "### Install Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "id": "82ad0c445061",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shapely<2.0.0 in /opt/conda/lib/python3.10/site-packages (1.8.5.post1)\n",
      "Requirement already satisfied: google-cloud-aiplatform in /home/jupyter/.local/lib/python3.10/site-packages (1.36.4)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.34.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.22.3)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.19.6)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (23.2)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.13.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.13.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.10.4)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.8.5.post1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.61.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.23.4)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.48.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.48.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.6.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.6)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.10/site-packages (from grpcio<2.0dev,>=1.33.2->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2023.7.22)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"shapely<2.0.0\"\n",
    "!pip install google-cloud-aiplatform --upgrade --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cebd6983cbad"
   },
   "source": [
    "**Colab only:** Uncomment the following cell to restart the kernel or use the button to restart the kernel. For Vertex AI Workbench you can restart the terminal using the button on top. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bea801acf6b5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a386d25fa8f"
   },
   "source": [
    "### Authenticating your notebook environment\n",
    "* If you are using **Colab** to run this notebook, uncomment the cell below and continue.\n",
    "* If you are using **Vertex AI Workbench**, check out the setup instructions [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bd1dca8e9a7"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Colab only:** Uncomment the following cell to initialize the Vertex AI SDK. For Vertex AI Workbench, you don't need to run this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import vertexai\n",
    "\n",
    "# PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "# vertexai.init(project=PROJECT_ID, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "PyQmSRbKA8r-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UP76a2la7O-a"
   },
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7isig7e07O-a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Settings\n",
    "Use these two settings to control the length of your output:\n",
    "\n",
    "**Output Length:** This is the max number of tokens that can be generated in the response. The higher the limit, the more likely the generated response is to veer off into unexpected territory. Consider shortening the output length for a more predictable response.\n",
    "\n",
    "**Add stop sequence:** This tells the model when to stop generating content. For example, prompting the model with ‚ÄúThe quick brown fox jumps over the ‚Äù and entering a full stop (.) for the stop sequence will tell the model to stop generating text once it reaches the end of the first sentence, regardless of the output length limit - in this case, the output will likely be \"lazy dog.\" Use this where you have a couple of examples, but would like the output to follow a certain pattern. For example, you can tell the model to generate lists that have no more than 10 items by adding \"11\" as a stop sequence.\n",
    "\n",
    "### These three settings will help you tweak how random the output will be:\n",
    "\n",
    "**Temperature:** Helps adjust how creative or conservative the model should be. It adjusts the probabilities of the predicted words, on a scale between 0 and 1. Lower temperatures (<0.5) are good for prompts that require higher likelihood of accuracy - a temperature of 0 is deterministic, where the probability of the most likely outcome (token) is 1, and the rest are 0. Use a higher temperature (>0.5) to generate more creative, \"fun\" results. We will use 0.1 as a standard best practice.\n",
    "\n",
    "**topK:** Tells the model to pick the next token from the top k tokens in its list, sorted by probability. A topK value of 40 tells the model to pick from the top 40 tokens, and exclude the rest.\n",
    "\n",
    "**topP:** Randomly samples from the top tokens based on the sum of their probabilities. In MakerSuite, the default topP is 0.95, meaning the model will pick from tokens whose probabilities add up to 95%, and exclude the bottom 5%.\n",
    "\n",
    "You can play around with these settings depending on your needs and use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIPcn5dZ7O-b"
   },
   "source": [
    "## Prompt engineering best practices (a lot more at the end!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df7d153f4928"
   },
   "source": [
    "Prompt engineering is all about how to design your prompts so that the response is what you were indeed hoping to see.\n",
    "\n",
    "The idea of using \"unfancy\" prompts is to minimize the noise in your prompt to reduce the possibility of the LLM misinterpreting the intent of the prompt. Below are a few guidelines on how to engineer \"unfancy\" prompts.\n",
    "\n",
    "In this section, you'll cover the following best practices when engineering prompts:\n",
    "\n",
    "* Be concise\n",
    "* Be specific, and well-defined\n",
    "* Ask one task at a time\n",
    "* Improve response quality by including examples\n",
    "* Turn generative tasks to classification tasks to improve safety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43c1169ac435"
   },
   "source": [
    "### Be concise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0f380f1620e"
   },
   "source": [
    "üõë Not recommended. The prompt below is unnecessarily verbose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "b6a1697c3603",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* **Dried & Lovely**\n",
      "* **Dried Blooms**\n",
      "* **Dried Florals**\n",
      "* **Dried Arrangements**\n",
      "* **Preserved Petals**\n",
      "* **Forever Flowers**\n",
      "* **Timeless Blooms**\n",
      "* **Dried Flower Boutique**\n",
      "* **Dried Flower Shop**\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What do you think could be a good name for a flower shop that specializes in selling bouquets of dried flowers more than fresh flowers? Thank you!\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2307f56a9b75"
   },
   "source": [
    "‚úÖ Recommended. The prompt below is to the point and concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "fc666404f47c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* **Dried Blooms**\n",
      "* **Preserved Petals**\n",
      "* **Forever Flowers**\n",
      "* **Dried & Wild**\n",
      "* **Naturally Beautiful**\n",
      "* **Rustic Blooms**\n",
      "* **Whimsical Bouquets**\n",
      "* **Sentimental Arrangements**\n",
      "* **One-of-a-kind Flowers**\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Suggest a name for a flower shop that sells bouquets of dried flowers\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17f6c48bba91"
   },
   "source": [
    "### Be specific, and well-defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "269b428e1563"
   },
   "source": [
    "Suppose that you want to brainstorm creative ways to describe Earth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6436ee2ff406"
   },
   "source": [
    "üõë Not recommended. The prompt below is too generic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "261b7f6e94c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earth is the third planet from the Sun, and the only astronomical object known to harbor life. It is the densest and fifth-largest of the eight planets in the Solar System. Earth's atmosphere is composed of 78% nitrogen, 21% oxygen, 1% other gases, and water vapor. The Earth's surface is divided into several tectonic plates that move around the planet's surface. The Earth's interior is divided into a solid inner core, a liquid outer core, and a mantle. The Earth's magnetic field is generated by the motion of the liquid outer core. The Earth's orbit around the Sun takes 365.256 days, or one year. The Earth's axis is tilted at an angle of 23.5 degrees, which causes the seasons. The Earth's rotation period is 24 hours, or one day.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me about Earth\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bebfecd2912"
   },
   "source": [
    "‚úÖ Recommended. The prompt below is specific and well-defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "242b1b3bae6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* **Earth is the only planet known to support life.** This is due to a number of factors, including its distance from the sun, its atmosphere, and its water.\n",
      "* **Earth has a large moon.** The moon is thought to have played a role in the development of life on Earth by stabilizing the planet's rotation and providing a source of tides.\n",
      "* **Earth has a relatively thin atmosphere.** This atmosphere protects the planet from harmful radiation from the sun and helps to keep the planet's temperature at a level that is conducive to life.\n",
      "* **Earth has a large amount of water.** Water is essential for life, and Earth has more water than any other planet in the solar system.\n",
      "* **Earth has a variety of landforms.** These landforms provide a variety of habitats for life, and they also help to regulate the planet's climate.\n",
      "* **Earth has a complex and diverse ecosystem.** This ecosystem is made up of a variety of organisms that interact with each other and with their environment. The ecosystem is essential for the survival of life on Earth.\n",
      "\n",
      "These are just a few of the ways that make Earth unique compared to other planets. Earth is a truly special planet, and it is home to a vast and amazing array of life.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Generate a list of ways that makes Earth unique compared to other planets\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20dca9a05eab"
   },
   "source": [
    "### Ask one task at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9019d443179"
   },
   "source": [
    "üõë Not recommended. The prompt below has two parts to the question that could be asked separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "70b3b5e5825d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best method of boiling water is to use a kettle. A kettle is a pot with a spout that is designed specifically for boiling water. It is typically made of metal, and has a lid that fits snugly to prevent steam from escaping. To boil water in a kettle, fill the kettle with water and place it on the stove. Turn the heat to high and wait for the water to boil. Once the water is boiling, turn off the heat and carefully pour the boiling water into a cup or mug.\n",
      "\n",
      "The sky is blue because of a phenomenon called Rayleigh scattering. Rayleigh scattering is the scattering of light by particles that are much smaller than the wavelength of light. In the case of the sky, the particles that are responsible for Rayleigh scattering are molecules of nitrogen and oxygen. These molecules are so small that they are much smaller than the wavelength of visible light. When sunlight hits these molecules, it is scattered in all directions. However, the blue light is scattered more than the other colors of light because it has a shorter wavelength. This is why the sky appears blue during the day.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What's the best method of boiling water and why is the sky blue?\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7936fb58c16a"
   },
   "source": [
    "‚úÖ Recommended. The prompts below asks one task a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "2564dad6c8db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best method of boiling water is to use a kettle. A kettle is a pot with a spout that is designed specifically for boiling water. Kettles are typically made of metal, and they have a lid that helps to keep the heat in. The spout allows you to pour the water easily without spilling it.\n",
      "\n",
      "To boil water in a kettle, fill the kettle with water and place it on the stove. Turn the heat to high and wait for the water to boil. Once the water is boiling, turn off the heat and carefully pour the water into a cup or mug.\n",
      "\n",
      "Boiling water in a kettle is the best method because it is quick and easy. Kettles are also relatively inexpensive, and they can be found at most stores.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What's the best method of boiling water?\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "770c695ade92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky is blue because of a phenomenon called Rayleigh scattering. This is the scattering of light by particles that are smaller than the wavelength of light. In the case of the sky, the particles that are doing the scattering are molecules of nitrogen and oxygen.\n",
      "\n",
      "When sunlight hits these molecules, the blue light is scattered more than the other colors. This is because the blue light has a shorter wavelength than the other colors. The red light, orange light, and yellow light are not scattered as much, so they travel in a straight line to our eyes. This is why we see the sky as blue.\n",
      "\n",
      "The amount of scattering depends on the wavelength of light and the size of the particles. The shorter the wavelength of light, the more it is scattered. The smaller the particles, the more they scatter light.\n",
      "\n",
      "This is why the sky is blue during the day. However, at sunrise and sunset, the sunlight has to travel through more of the atmosphere to reach our eyes. This means that more of the blue light is scattered, and we see the sky as more red or orange.\n",
      "\n",
      "The sky can also appear blue when there is a lot of dust or pollution in the air. This is because the dust and pollution particles act as additional scattering particles.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Why is the sky blue?\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff606011aa86"
   },
   "source": [
    "### Watch out for hallucinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "956ce45b06a7"
   },
   "source": [
    "Although LLMs have been trained on a large amount of data, they can generate text containing statements not grounded in truth or reality; these responses from the LLM are often referred to as \"hallucinations\" due to their limited memorization capabilities. Note that simply prompting the LLM to provide a citation isn‚Äôt a fix to this problem, as there are instances of LLMs providing false or inaccurate citations. Dealing with hallucinations is a fundamental challenge of LLMs and an ongoing research area, so it is important to be cognizant that LLMs may seem to give you confident, correct-sounding statements that are in fact incorrect. \n",
    "\n",
    "Note that if you intend to use LLMs for the creative use cases, hallucinating could actually be quite useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c9d5f66179a"
   },
   "source": [
    "Try the prompt like the one below repeatedly. You may notice that sometimes it will confidently, but inaccurately, say \"The first elephant to visit the moon was Luna\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "d813b9061b08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the article, the first elephant to visit the moon was Luna.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who was the first elephant to visit the moon?\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "029e23abfd56"
   },
   "source": [
    "### Turn generative tasks into classification tasks to reduce output variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d943941d6e59"
   },
   "source": [
    "#### Generative tasks lead to higher output variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37528e6c9754"
   },
   "source": [
    "The prompt below results in an open-ended response, useful for brainstorming, but response is highly variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "a8e2dc39e9ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* **Write a program to solve a problem you're interested in.** This could be anything from a game to a tool to help you with your studies. The important thing is that you're interested in the problem and that you're motivated to solve it.\n",
      "* **Take a programming course.** There are many online and offline courses available, so you can find one that fits your schedule and learning style.\n",
      "* **Join a programming community.** There are many online and offline communities where you can connect with other programmers and learn from each other.\n",
      "* **Read programming books and articles.** There is a wealth of information available online and in libraries about programming. Reading about different programming languages and techniques can help you improve your skills.\n",
      "* **Practice, practice, practice!** The best way to improve your programming skills is to practice as much as you can. The more you code, the better you'll become.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I'm a high school student. Recommend me a programming activity to improve my skills.\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f71a6fa2b4bb"
   },
   "source": [
    "#### Classification tasks reduces output variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "917517465dac"
   },
   "source": [
    "The prompt below results in a choice and may be useful if you want the output to be easier to control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "3feb93d9df81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would suggest learning Python. Python is a general-purpose programming language that is easy to learn and has a wide range of applications. It is used for web development, data science, machine learning, and many other things. Python is also a popular language for beginners, as it has a large community of support and resources available.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"I'm a high school student. Which of these activities do you suggest and why:\n",
    "a) learn Python\n",
    "b) learn Javascript\n",
    "c) learn Fortran\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32290ac9fb2b"
   },
   "source": [
    "### Improve response quality by including examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "132834f5db2c"
   },
   "source": [
    "Another way to improve response quality is to add examples in your prompt. The LLM learns in-context from the examples on how to respond. Typically, one to five examples (shots) are enough to improve the quality of responses. Including too many examples can cause the model to over-fit the data and reduce the quality of responses.\n",
    "\n",
    "Similar to classical model training, the quality and distribution of the examples is very important. Pick examples that are representative of the scenarios that you need the model to learn, and keep the distribution of the examples (e.g. number of examples per class in the case of classification) aligned with your actual distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46520d938b6a"
   },
   "source": [
    "#### Zero-shot prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46d3b47e6cea"
   },
   "source": [
    "Below is an example of zero-shot prompting, where you don't provide any examples to the LLM within the prompt itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "2cbe03eb0b71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1:\n",
      "* Morning: Start your day with a hearty breakfast at the Cinnamon Club, a highly-rated Indian restaurant in Pimlico. Be sure to try the chicken tikka masala or the lamb vindaloo.\n",
      "* Afternoon: After breakfast, take the Underground to the National Gallery. This world-renowned art museum is home to some of the most famous paintings in the world, including the Arnolfini Portrait by Jan van Eyck.\n",
      "* Evening: For dinner, head to the Jazz Cafe in Camden Town. This legendary music venue has been hosting live jazz since the 1960s.\n",
      "\n",
      "Day 2:\n",
      "* Morning: Start your day with a walk through Hyde Park. This beautiful park is home to a variety of attractions, including the Serpentine Lake, the Diana Memorial Fountain, and the Speaker's Corner.\n",
      "* Afternoon: After your walk, take the Underground to the British Museum. This massive museum is home to over 8 million artifacts from all over the world.\n",
      "* Evening: For dinner, head to the Dishoom in Covent Garden. This popular Indian restaurant is known for its delicious curries and its lively atmosphere.\n",
      "\n",
      "Day 3:\n",
      "* Morning: Start your day with a visit to the Tower of London. This historic castle is home to the Crown Jewels, the Tower Bridge, and the Bloody Tower.\n",
      "* Afternoon: After your visit to the Tower of London, take the Underground to Borough Market. This bustling market is a great place to find fresh produce, meats, cheeses, and breads.\n",
      "* Evening: For dinner, head to the Smokehouse in Shoreditch. This gastropub is known for its wood-fired cooking and its extensive craft beer selection.\n",
      "\n",
      "This itinerary will give you a taste of some of the best that London has to offer. Be sure to do some research before you go to find other attractions and restaurants that interest you. And don't forget to bring your camera!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"I'll be staying in London for 72 hours, in Pimlico. It'll be near the\n",
    "end of my trip, so I'll be kinda broke. I love hot curries, and hate fish. I\n",
    "love live music and hate tourist traps. I want to make sure I see the Arnolfini\n",
    "Portrait before leaving. I'll be using the Underground to travel. Please\n",
    "recommend an itinerary with detailed travel routes and suggestions for meals.\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the difference between telling the model what not to do (\"Don't suggest fish restaurants\"), and offering detailed, straightforward language about personal preferences.\n",
    "\n",
    "While it's generally best practice to be as direct as possible, it's interesting to observe how easily the model can infer a request for budget restaurant options from colloquial language like \"kinda broke,\" while also avoiding recommendations for fish and chip shops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen an example of a zero-shot prompt. A system prompt is a specific type of prompt that provides the model with additional instructions on how to generate the output. It contains suggestions to the style, format, or content of the output, whereas a normal prompt tends to be more open-ended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Classify movie reviews as positive, neutral or negative. Only return the label in uppercase.\n",
    "Review: ‚ÄúHer‚Äù is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. It's so disturbing I couldn't watch it.\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "System prompts can be useful for generating output that meets specific requirements. The name system prompt actually stands for \"providing an additional task to the system\". For example, you could use a system prompt to generate a code snippet that is compatible with a specific programming language, or you could use a system prompt to return a certain structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"movie_reviews\": [\n",
      "    {\n",
      "      \"sentiment\": \"NEGATIVE\",\n",
      "      \"name\": \"Her\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Classify movie reviews as positive, neutral or negative. Return valid JSON:\n",
    "Review: ‚ÄúHer‚Äù is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. It's so disturbing I couldn't watch it.\n",
    "\n",
    "Schema:\n",
    "MOVIE:\n",
    "{\n",
    "\"sentiment\": String POSITIVE | NEGATIVE | NEUTRAL,\n",
    "\"name\": String\n",
    "}\n",
    "\n",
    "{\n",
    "\"movie_reviews\": [MOVIE]\n",
    "}\n",
    "\n",
    "JSON Response:\n",
    "```json\n",
    "```\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some benefits in returning JSON objects from a prompt. In a real-world application I don't need to manually create this JSON format, but secondly, by prompting for a JSON format it forces the model to create a structure and limit hallucinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0daabca1359"
   },
   "source": [
    "#### One-shot prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42c4652fc5c2"
   },
   "source": [
    "Below is an example of one-shot prompting, where you provide one example to the LLM within the prompt to give some guidance on what type of response you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "cfe584860787"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Decide whether a Tweet's sentiment is positive, neutral, or negative.\n",
    "\n",
    "Tweet: I loved the new YouTube video you made!\n",
    "Sentiment: positive\n",
    "\n",
    "Tweet: That was awful. Super boring üò†\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef58c35005c0"
   },
   "source": [
    "#### Few-shot prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b630e8947b60"
   },
   "source": [
    "While large-language models demonstrate remarkable zero-shot capabilities, they still fall short on more complex tasks when using the zero-shot setting. Few-shot prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance. The demonstrations serve as conditioning for subsequent examples where we would like the model to generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "fb3ba21bbd11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kids were farduddling around the room, laughing and having fun.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses\n",
    "the word whatpu is:\n",
    "We were traveling in Africa and we saw these very cute whatpus.\n",
    "To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses\n",
    "the word farduddle is:\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Role prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Role prompting is a technique in prompt engineering that involves assigning a specific role to the generative AI model. This can help the model to generate more relevant and informative output, as the model can craft its responses to the specific role that it has been assigned.\n",
    "\n",
    "For example, you could role prompt a generative AI model to be a book editor, a kindergarten teacher, or a motivational speaker. Once the model has been assigned a role, you can then give it prompts that are specific to that role.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pizza place] is a great place to get pizza. The pizza is delicious and the service is excellent. The staff is friendly and attentive, and they always make sure that your pizza is cooked to perfection. I highly recommend [pizza place] to anyone who loves pizza.\n",
      "\n",
      "One of the things that I love about [pizza place] is the variety of pizzas that they offer. They have everything from classic pepperoni pizza to more unique pizzas like the Hawaiian pizza and the Buffalo chicken pizza. No matter what your taste, you're sure to find a pizza that you love at [pizza place].\n",
      "\n",
      "The service at [pizza place] is also excellent. The staff is always friendly and attentive, and they always make sure that your pizza is cooked to perfection. They're also very quick, and your pizza will usually be ready in just a few minutes.\n",
      "\n",
      "Overall, I highly recommend [pizza place]. It's a great place to get delicious pizza and excellent service.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Write a review of [pizza place].\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've been to a lot of pizza places in my time, but [random pizza place] is by far the best. The pizza is made with fresh, high-quality ingredients, and the crust is perfectly crispy and chewy. The sauce is tangy and flavorful, and the cheese is gooey and melted. I've tried a variety of pizzas from [random pizza place], and they've all been delicious. My personal favorite is the pepperoni pizza, but the cheesesteak pizza is also amazing.\n",
      "\n",
      "The service at [random pizza place] is also top-notch. The staff is friendly and attentive, and they always make sure that your food is cooked to perfection. They also have a great selection of beers and wines to choose from.\n",
      "\n",
      "If you're looking for a truly memorable pizza experience, then I highly recommend [random pizza place]. You won't be disappointed.\n",
      "\n",
      "Here are some specific details about my experience at [random pizza place]:\n",
      "\n",
      "* The atmosphere is casual and relaxed.\n",
      "* The service is friendly and attentive.\n",
      "* The menu has a wide variety of pizzas to choose from, including classic pizzas, specialty pizzas, and vegetarian pizzas.\n",
      "* The pizzas are made with fresh, high-quality ingredients.\n",
      "* The crust is perfectly crispy and chewy.\n",
      "* The sauce is tangy and flavorful.\n",
      "* The cheese is gooey and melted.\n",
      "* The prices are reasonable.\n",
      "* The beers and wines are a great complement to the pizza.\n",
      "\n",
      "Overall, I had a wonderful experience at [random pizza place]. The pizza was delicious, the service was excellent, and the atmosphere was perfect. I highly recommend this restaurant to anyone who loves pizza.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"You are a food critic. Write a review of [random pizza place].\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a tone and perspective for an AI model gives it a blueprint of the tone, style, and focused expertise you‚Äôre looking for to improve the quality, relevance, and effectiveness of your output. \n",
    "\n",
    "Here are some styles you can choose from which I find effective:\n",
    "Confrontational, Descriptive, Direct, Formal, Humorous, Influential, Informal, Inspirational, Persuasive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've been to a lot of pizza places in my time, but I've never seen one quite like [random pizza place]. The first thing you notice when you walk in is the smell. It's not a bad smell, exactly, but it's definitely...unique.\n",
      "\n",
      "The pizza itself is pretty good, but it's not the kind of pizza you'd want to eat every day. It's a little on the greasy side, and the sauce is a bit too sweet. But the crust is really good, and the toppings are fresh.\n",
      "\n",
      "The service at [random pizza place] is also pretty good. The staff is friendly and attentive, and they're always happy to make recommendations.\n",
      "\n",
      "Overall, I'd say [random pizza place] is a pretty good place to get pizza. It's not the best pizza I've ever had, but it's definitely worth a try.\n",
      "\n",
      "But here's the thing: [random pizza place] is also a really weird place. I mean, they have a whole section of their menu dedicated to pizzas with toppings like \"peanut butter and jelly\" and \"Cheetos and hot sauce.\" And they have a drink called \"The Screaming Goat\" that's made with goat milk, red wine, and cinnamon.\n",
      "\n",
      "I'm not sure what the owner of [random pizza place] is thinking, but I'm glad they're doing it. This place is a one-of-a-kind experience, and I'm glad I had the chance to try it.\n",
      "\n",
      "So if you're ever in the mood for something a little different, I highly recommend checking out [random pizza place]. Just be prepared for a weird experience.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"You are a food critic. Write a humorous review of [random pizza place].\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaint of Thought"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain of Thought (CoT) [6] prompting is a technique which was first introduced in January 2022 for improving the reasoning capabilities of LLMs by letting them write intermediate reasoning steps. This helps the LLM understand the problem more deeply, and generate more accurate answers. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Lets first look at the example without CoT\n",
    "\n",
    "prompt = \"\"\"When I was 3 years old, my partner was 3 times my age. Now, I am 20 years old. How old is my partner?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When I was 3 years old, my partner was 3 * 3 = 9 years old. Now, my partner is 20 + 9 = 29 years old.\n",
      "The final answer: 29.\n"
     ]
    }
   ],
   "source": [
    "#Now, lets add CoT\n",
    "\n",
    "prompt = \"\"\" When I was 3 years old, my partner was 3 times my age. Now, I am 20 years old. How old is my partner? Let's think step by step.\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain of Thought prompting can be very powerful when combined with a single-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When I was 3 years old, my partner was 3 * 3 = 9 years old. Now I am 20 years old, so my partner is 20 + 9 = 29 years old. The answer is 29.\n"
     ]
    }
   ],
   "source": [
    "#Now, lets add CoT\n",
    "\n",
    "prompt = \"\"\" Q: When my brother was 4 years old, I was double his age. Now I am 40 years old. How old is my brother? Let's think step by step.\n",
    "A: When my brother was 4 years, I was 2 * 2 = 8 years old. That's an age difference of 4 years and I am older. Now I am 40 years old, so my brother is 40 - 4  = 36 years old. The answer is 36.\n",
    "Q: When I was 3 years old, my partner was 3 times my age. Now, I am 20 years old. How old is my partner? Let's think step by step.\n",
    "A:\n",
    "\"\"\"\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree of Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are familiar with Chain of Thoughts, let's review Tree of Thoughts (ToT) , which was first introduced in May 2023. It generalizes the concept of CoT prompting because it allows LLMs to explore multiple different reasoning paths simultaneously, rather than just following a single linear chain of thought.\n",
    "\n",
    "![Tree of Thoughts](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2FTOT.3b13bc5e.png&amp;w=3840&amp;q=75)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Expert 1:**\n",
      "\n",
      "When I was 3 years old, my partner was 3 times my age, so my partner was 3 * 3 = 9 years old. Now, I am 20 years old, so my partner is 20 - 9 = 11 years older than me. Therefore, my partner is 20 + 11 = 31 years old.\n",
      "\n",
      "**Expert 2:**\n",
      "\n",
      "When I was 3 years old, my partner was 3 times my age, so my partner was 3 * 3 = 9 years old. Now, I am 20 years old, so my partner is 20 - 9 = 11 years older than me. However, my partner cannot be 11 years old because they were already 9 years old when I was 3 years old. Therefore, my partner must be 11 + 9 = 20 years old.\n",
      "\n",
      "**Expert 3:**\n",
      "\n",
      "When I was 3 years old, my partner was 3 times my age, so my partner was 3 * 3 = 9 years old. Now, I am 20 years old, so my partner is 20 - 9 = 11 years older than me. However, my partner cannot be 11 years old because they were already 9 years old when I was 3 years old. Therefore, my partner must be 11 + 9 = 20 years old.\n",
      "\n",
      "**The correct answer is 20 years old.**\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The question is: \"When I was 3 years old, my partner was 3 times my age. Now, I am 20 years old. How old is my partner?\"\n",
    "\n",
    "Now imagine 3 different experts are answering this question.\n",
    "All experts will write down the steps of their thinking, by calculating the age difference back then to calculate the current age of the partner. If any expert realizes they're wrong at any point then they leave. \n",
    "The correct answer is the answer that has been answered the most.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reason and Act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reason and Act (ReAct) [8] prompting was first introduced in October 2022 and is a paradigm for enabling LLMs to solve complex tasks using natural language reasoning. It is designed for tasks where the LLM is allowed to perform certain actions, such as interacting with external APIs to retrieve information. \n",
    "ReAct mimics how humans operate in the real world, as we reason verbally and can take actions to gain information. ReAct performs well against other prompt engineering approaches in a variety of domains.\n",
    "\n",
    "ReAct prompting works by combining reasoning and acting into a thought-action loop. The LLM first reasons about the problem and generates a plan of action. It then performs the actions in the plan and observes the results. The LLM then uses the observations to update its reasoning and generate a new plan of action. This process continues until the LLM reaches a solution to the problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.10/site-packages (0.0.344)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.8.6)\n",
      "Requirement already satisfied: anyio<4.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-core<0.1,>=0.0.8 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.0.8)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.0.67)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.10.13)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.1.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: google-search-results in /opt/conda/lib/python3.10/site-packages (2.4.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from google-search-results) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->google-search-results) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->google-search-results) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->google-search-results) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->google-search-results) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting numexpr\n",
      "  Downloading numexpr-2.8.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.10/site-packages (from numexpr) (1.24.4)\n",
      "Downloading numexpr-2.8.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m384.1/384.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numexpr\n",
      "Successfully installed numexpr-2.8.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain\n",
    "%pip install google-search-results\n",
    "%pip install numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Leo DiCaprio's current girlfriend is Camila Morrone.\n",
      "Action: Search\n",
      "Action Input: Camila Morrone age\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m26 years\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m 26 ^ 0.43 = 13.49\n",
      "Action: Calculator\n",
      "Action Input: 26 ^ 0.43\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 4.059182145592686\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m The answer is 4.059182145592686\n",
      "Final Answer: 4.059182145592686\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4.059182145592686'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import VertexAI\n",
    "\n",
    "prompt = \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n",
    "\n",
    "llm = VertexAI(temperature=0.1)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "agent = initialize_agent(tools, llm,  agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "agent.run(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Automatic Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you might realize that writing a prompt can be complex and. Wouldn't it be possible to automate this (write a prompt to write prompts) and take out natural language ambiguity? Well, there's a method: Automatic Prompt Engineering (APE). . This method - described in a 2023 research paper  - not only alleviates the need for human input but also enhances the model‚Äôs performance in various tasks.\n",
    "\n",
    "The idea is at follows:\n",
    "\n",
    "You pick a task that needs to automate the prompt for crafting and optimizing the best prompt for you. \n",
    "\n",
    "In our example, we are building a shopping assistant for a retail company. We want to figure out all the various ways customers could phrase their order for buying a new coat.\n",
    "\n",
    "Write the prompt which will generate the instruction candidates. You can do this through an LLM, but there are also various hubs and collections of prompts that you can find online. In this example, I am using Text-Bison-32k to generate 10 instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. I would like to order a black puffer jacket in size M.\n",
      "2. Can you please order me a black puffer jacket in size M?\n",
      "3. I'm looking for a black puffer jacket in size M. Can you help me find one?\n",
      "4. I need a black puffer jacket in size M. Can you order it for me?\n",
      "5. I'm interested in buying a black puffer jacket in size M. Can you help me with that?\n",
      "6. I would like to purchase a black puffer jacket in size M. Can you help me with that?\n",
      "7. I'm looking to buy a black puffer jacket in size M. Can you help me find one?\n",
      "8. I'm in the market for a black puffer jacket in size M. Can you help me find one?\n",
      "9. I'm looking to order a black puffer jacket in size M. Can you help me with that?\n",
      "10. I'm looking to purchase a black puffer jacket in size M. Can you help me with that?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"We have a retail webshop, and to train a chatbot we need various ways to order: \"Coat, black, puffer jacket size M\". Generate 10 variants, with the same semantics but keep the same meaning.\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a step-back prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen with most prompting techniques published, Large Language Models (LLMs) need guidance when intricate, multi-step reasoning is demanded from a query, and decomposition is a key component when solving complex request.\n",
    "\n",
    "A process of supervision with step-by-step verification is a promising remedy to improve the correctness of intermediate reasoning step\n",
    "\n",
    "The most well known prompting technique when it comes to decomposition is chain-of-thought reasoning. In this study Step-Back Prompting is compared to COT prompting.\n",
    "\n",
    "The text below shows a complete example of STP with the original question, the stepback question, principles, and the prompt for the final answer to be generated by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stepback Question: Who was the president of the United States when Palm was announced?\n"
     ]
    }
   ],
   "source": [
    "#This shows example of the stepback question that is generated\n",
    "\n",
    "prompt = \"\"\" You are an expert at world knowledge. \n",
    "Your task is to step back and paraphrase a question to a more generic \n",
    "step-back question, which is easier to answer. \n",
    "\n",
    "Here are a few examples:\n",
    "Original Question: Which position did Knox Cunningham hold from May 1955 to Apr 1956?\n",
    "Stepback Question: Which positions have Knox Cunning- ham held in his career?\n",
    "\n",
    "Original Question: Who was the spouse of Anna Karina from 1968 to 1974?\n",
    "Stepback Question: Who were the spouses of Anna Karina?\n",
    "\n",
    "Original Question: Which team did Thierry Audel play for from 2007 to 2008?\n",
    "Stepback Question: Which teams did Thierry Audel play for in his career?\n",
    "\n",
    "Question: Was donald trump president when Palm was announced?\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"did Google Bard exist while Trump was president? Answer with yes or no and provide your explanation.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Yes. \\n\\nGoogle Bard was announced by Sundar Pichai, the CEO of Alphabet Inc., on May 11, 2022. Donald Trump's presidency ended on January 20, 2021. \\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.utilities import SerpAPIWrapper\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "\n",
    "def retriever(query):\n",
    "    return search.run(query)\n",
    "\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "{normal_context}\n",
    "\n",
    "Original Question: {question}\n",
    "Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "######################################\n",
    "\n",
    "chain = {\n",
    "    # Retrieve context using the normal question\n",
    "    \"normal_context\": RunnableLambda(lambda x: x['question']) | retriever,\n",
    "    # Pass on the question\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "} | response_prompt | ChatVertexAI(temperature=0) | StrOutputParser()\n",
    "\n",
    "######################################\n",
    "\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4023be726eb"
   },
   "source": [
    "### Other best practices:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6d7870ff75cc"
   },
   "source": [
    "**Emphasis through prompting:**\n",
    "Emphasis through Capitalization: Use capitalization to stress important points or instructions. This can help guide the model's responses and maintain a certain focus. For instance, \"Remember to KEEP DETAILS CONFIDENTIAL\".\n",
    "\n",
    "**Extreme Emphasis:** \n",
    "You can use exaggerations or hyperbolic language to amplify the importance of your prompt or instructions. For instance, instead of saying \"Make sure it's clear,\" you might say, \"Your explanation should be as clear as a summer's day, absolutely impossible to misinterpret. Every single word must ooze clarity!\"\n",
    "\n",
    "**Emphasis through Repetition:**\n",
    "Reiteration of key phrases or ideas can effectively guide the AI model's responses. For example, \"remember to be creative... keep in mind to be creative... don't forget to be creative...\"\n",
    "\n",
    "**Prompt modularization:**\n",
    "Modularize instruction: Break down complex tasks into a sequence of simpler prompts, facilitating more interactive and manageable conversations. Ex: Instead of having many instructions in 1 prompt, modularize the prompts and have 1 prompt per instruction. Then based on the user‚Äôs  input, you can choose which prompt to process.\n",
    "\n",
    "**Prompt chaining:** \n",
    "This is a technique to complete complex tasks by chaining together multiple prompts. The output of one prompt is used as the input for the next prompt, and so on. For example, first convert the file format to csv, then do operation x, then do operation y on the new dataset and so on.\n",
    "\n",
    "**Aggregation technique:**\n",
    "This is a technique to perform certain tasks on specific portions of the data and then aggregate them all to produce the final output. Ex: Do operation x on the first part of the data, do operation y on the rest of the data and aggregate the results. \n",
    "\n",
    "#### Trial and error:\n",
    "**Iterative prompting:** \n",
    "Technique where prompts are continuously refined and adjusted to enhance clarity, effectiveness, and quality when working with large language models; by progressively adapting prompts based on feedback, it ensures that responses meet desired criteria; this is especially useful in application development and content generation, optimizing interaction between human input and machine learning models for better outcomes.\n",
    "\n",
    "**Starting over:**\n",
    "Sometimes, when a prompt is not working well, and some iterations of it are not doing it, start from scratch with a different prompt, see which sections from the previous prompt help, which don‚Äôt - filter out the sections which ‚Äúwork‚Äù (make the model behave the way you want) and add to that. \n",
    "\n",
    "**Modular testing:**\n",
    "When a prompt is not working, split the task into many sub tasks and run the portion of the prompt that achieves a given sub-task. This will help you identify where it is not working and fix the prompt accordingly.\n",
    "\n",
    "#### Reinforcement techniques:\n",
    "**Redundancy Technique:**\n",
    "Similar to the \"Capitalize vs Repeat\" technique, use synonyms or different phraseologies for the same instruction to reinforce the direction of the prompt. Ex: Instead of saying ‚Äòmake sure to..‚Äô you say the same thing many times by replacing ‚Äòmake sure‚Äô with ‚Äòensure‚Äô, ‚Äòguarantee‚Äô, ‚Äòverify‚Äô..  \n",
    "\n",
    "**Negative Instructions:**\n",
    "Explicitly mention what you don't want in the response. This is useful when the model tends to produce certain unwanted outputs for specific prompts. Ex: ‚ÄòDon‚Äôt output any sources.‚Äô\n",
    "\n",
    "**Self-reference:**\n",
    "Use the AI's self-awareness to your advantage by instructing it to evaluate or check its own responses before producing them. For example, \"Make sure your response is concise and does not include unnecessary details\".\n",
    "\n",
    "#### Prompt formatting techniques:\n",
    "**Structure:**\n",
    "Having a structured prompt helps the model better understand the task. Start by defining its role. Ex: ‚ÄúYour role is to read the input data and structure it in the following format‚Äù Then tell it what the input data is, then tell it what the desired format is, then tell it how to do it.\n",
    "\n",
    "**Use constraints:** \n",
    "Constraints can be used to limit the scope of the model's output. For example, you could constrain the model to generate a line that is no longer than 10 words long or not to output any other data than the input data. \n",
    "\n",
    "**Use delimiters:** \n",
    "Delimiters in prompt engineering provide clear distinctions within your text, effectively separating instructions from content. This practice helps prevent misunderstandings, allows you greater control over AI responses, and ensures consistent results, even when handling complex tasks.\n",
    "\n",
    "#### Use LLMs (ex: Bard) to build prompts: \n",
    "**Refining Prompts:**\n",
    "By providing an initial prompt and the output received, you can ask the LLM to refine the prompt to achieve a more desired output. The LLM can provide insight into what might be improved, suggest changes\n",
    "\n",
    "**Generating New Prompts:**\n",
    "If you provide the LLM with a set of requirements, desired output format, and examples of \"requirement, prompt\" tuples, it can generate a new prompt based on the given requirements. This could be useful when creating prompts for new scenarios or tasks.\n",
    "\n",
    "**Feedback:** \n",
    "Once you have a prompt, you can ask the LLM for feedback. This can help you to identify areas where the prompt could be improved. The LLm can help you point out some gaps in your prompt engineering skills based on your prompt and what you want to achieve.\n",
    "\n",
    "#### Miscellaneous:\n",
    "**Prioritization:** \n",
    "Clearly communicate to the AI model what content or information is most critical. This approach helps in guiding the AI's focus and ensures the most important topics or issues are addressed accurately.\n",
    "\n",
    "**Use examples:**\n",
    "Providing examples of the desired output can also be helpful. This can help the model to understand what you are looking for and to generate more accurate results. Sometimes you can describe what you want so well that you won‚Äôt need an example, but it only helps the model to give it more information as long as you‚Äôre operating below the context window. \n",
    "\n",
    "**Preferred Output Format:**\n",
    "Specify your desired output format. If a CSV format is more useful than plain text, instruct the AI model to produce the output in that format from the outset. This strategy can save time and effort on manual conversions later. \n",
    "We have seen that the model processes csv and json format better than it does with txt files. \n",
    "Prompting it to first convert the txt file to csv or json yields a better result. \n",
    "Writing your prompt in json format or requesting the output to be in json or csv format yield more accurate results. \n",
    "\n",
    "**Use a prompt library:**\n",
    "There are a number of online libraries that contain pre-made prompts that can be used for a variety of tasks. We have a prompt library in Vertex AI ‚Üí Generative AI studio ‚Üí Language ‚Üí prompt examples. This can be a helpful resource if you‚Äôd like a starting point or have a very common task. \n",
    "\n",
    "**Nice words:**\n",
    "Using words like ‚Äòplease‚Äô, ‚Äògreat job!‚Äô, ‚Äòyou are the best!‚Äô ‚Ä¶ increases the model‚Äôs compliance rate. This increases the ‚Äòconfidence level‚Äô of the model and it positively reinforces the model‚Äôs behavior which tends to increase accuracy for simple tasks.\n",
    "\n",
    "**Lastly and most important: Be Creative:**\n",
    "This isn‚Äôt even the tip of the iceberg - the more creative you are, the better you can make the model behave the way you want. For example, instead of saying \"provide deep insights,\" you can write: ‚ÄúEnsure that you provide deep insights... Provide very very deep insights. I told you to give deep insights!!!!! Do not write anything that is not deeply insightful! Make sure to check this requirement is 100% met every time you respond. It‚Äôs better that you don‚Äôt return anything than returning something that‚Äôs not very very insightful!!!‚Äù. If you have a long prompt, you may also consider adding the same statement in different places - this helps the model prioritize that task (aka sandwich technique).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out https://gandalf.lakera.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Check out https://github.com/GoogleCloudPlatform/generative-ai.git for many more notebooks and exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cannot find the password? Tips: https://medium.com/the-abcs-of-ai/gandalfs-challenge-mastering-prompt-engineering-for-ai-success-fd777be2aa0b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_prompt_design.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
